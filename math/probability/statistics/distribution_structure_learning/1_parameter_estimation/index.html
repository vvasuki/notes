<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
    
    <title>Vishvas&#39;s notes  | 1 Parameter estimation</title>
    <meta name="HandheldFriendly" content="True">
    <meta name="MobileOptimized" content="320">

    <meta name="viewport" content="width=device-width,minimum-scale=1">
    <meta name="generator" content="Hugo 0.72.0" />
    
    
      <META NAME="ROBOTS" CONTENT="INDEX, FOLLOW">
    

    <link href="/notes/math/probability/statistics/distribution_structure_learning/1_parameter_estimation/" rel="alternate" type="application/rss+xml" title="Vishvas&#39;s notes" />
    <link href="/notes/math/probability/statistics/distribution_structure_learning/1_parameter_estimation/" rel="feed" type="application/rss+xml" title="Vishvas&#39;s notes" />

    <meta property="og:title" content="1 Parameter estimation" />
<meta property="og:description" content="Estimate parameters using statistics The distinction between choosing parametric and non-parametric approaches are considered in the decision theory section.
Statistic, estimator A statistic \(\hat{t} = \hat{g}(X)\) is a function of the sample \(X\); an observable random variable. When it is used to estimate some parameter, it is called an estimator. t can be estimated by estimating \(\gth\).
Point estimation of the parameter If \(\hat{t}\) tries to approximate \(t\), it is an estimator." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://vvasuki.github.io/notes/math/probability/statistics/distribution_structure_learning/1_parameter_estimation/" />

<meta itemprop="name" content="1 Parameter estimation">
<meta itemprop="description" content="Estimate parameters using statistics The distinction between choosing parametric and non-parametric approaches are considered in the decision theory section.
Statistic, estimator A statistic \(\hat{t} = \hat{g}(X)\) is a function of the sample \(X\); an observable random variable. When it is used to estimate some parameter, it is called an estimator. t can be estimated by estimating \(\gth\).
Point estimation of the parameter If \(\hat{t}\) tries to approximate \(t\), it is an estimator.">

<meta itemprop="wordCount" content="1413">



<meta itemprop="keywords" content="" /><meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="1 Parameter estimation"/>
<meta name="twitter:description" content="Estimate parameters using statistics The distinction between choosing parametric and non-parametric approaches are considered in the decision theory section.
Statistic, estimator A statistic \(\hat{t} = \hat{g}(X)\) is a function of the sample \(X\); an observable random variable. When it is used to estimate some parameter, it is called an estimator. t can be estimated by estimating \(\gth\).
Point estimation of the parameter If \(\hat{t}\) tries to approximate \(t\), it is an estimator."/>

      
    

    <script src="/notes/webpack_dist/dir_tree-bundle.js"></script>
    <script type="text/javascript">
    
    let baseURL = "https:\/\/vvasuki.github.io\/notes\/";
    let basePath = "/" + baseURL.split("/").slice(3).join("/");
    let siteParams = JSON.parse("{\u0022contactlink\u0022:\u0022https:\/\/github.com\/vvasuki\/notes\/issues\/new\u0022,\u0022disqusshortcode\u0022:\u0022vvasuki-site\u0022,\u0022env\u0022:\u0022production\u0022,\u0022githubeditmepathbase\u0022:\u0022https:\/\/github.com\/vvasuki\/notes\/edit\/hugo-source\/content\/\u0022,\u0022mainSections\u0022:[\u0022history\u0022],\u0022mainsections\u0022:[\u0022history\u0022],\u0022math\u0022:true}");
    let pageDefaultsList = JSON.parse("[{\u0022scope\u0022:{\u0022pathPrefix\u0022:\u0022\u0022},\u0022values\u0022:{\u0022comments\u0022:true,\u0022layout\u0022:\u0022page\u0022,\u0022search\u0022:true,\u0022sidebar\u0022:\u0022home_sidebar\u0022}}]");
    let sidebarsData = JSON.parse("{\u0022home_sidebar\u0022:{\u0022contents\u0022:[{\u0022url\u0022:\u0022recdir:\/\/\u0022},{\u0022contents\u0022:[{\u0022title\u0022:\u0022‡§ú‡•ç‡§Ø‡•å‡§§‡§ø‡§∑‡§Æ‡•ç\u0022,\u0022url\u0022:\u0022..\/jyotiSham\/\u0022},{\u0022title\u0022:\u0022‡§∏‡§Ç‡§∏‡•ç‡§ï‡•É‡§§‡§Æ‡•ç\u0022,\u0022url\u0022:\u0022..\/sanskrit\/\u0022},{\u0022title\u0022:\u0022‡§∏‡§Ç‡§∏‡•ç‡§ï‡§æ‡§∞‡§É\u0022,\u0022url\u0022:\u0022..\/saMskAra\/\u0022},{\u0022title\u0022:\u0022Notes Home\u0022,\u0022url\u0022:\u0022..\/notes\/\u0022},{\u0022title\u0022:\u0022‡§µ‡•á‡§¶‡§æ‡§É\u0022,\u0022url\u0022:\u0022..\/vedAH\/\u0022},{\u0022title\u0022:\u0022‡§™‡•Å‡§∞‡§æ‡§£‡§Æ‡•ç\u0022,\u0022url\u0022:\u0022..\/purANam\/\u0022},{\u0022title\u0022:\u0022‡§ï‡§æ‡§µ‡•ç‡§Ø‡§Æ‡•ç\u0022,\u0022url\u0022:\u0022..\/kAvyam\/\u0022},{\u0022title\u0022:\u0022‡§Æ‡•Ä‡§Æ‡§æ‡§Ç‡§∏‡§æ\u0022,\u0022url\u0022:\u0022..\/mImAMsA\/\u0022},{\u0022title\u0022:\u0022‡§§‡•ç‡§∞‡§ø‡§™‡§ø‡§ü‡§ï‡§Æ‡•ç\u0022,\u0022url\u0022:\u0022..\/tipiTaka\/\u0022},{\u0022title\u0022:\u0022‡§™‡§æ‡§≥‡§Ø‡§É\u0022,\u0022url\u0022:\u0022..\/pALi\/\u0022},{\u0022title\u0022:\u0022Vishvas\u0027s home page\u0022,\u0022url\u0022:\u0022\/..\/\u0022}],\u0022title\u0022:\u0022‡§∏‡§ô‡•ç‡§ó‡•ç‡§∞‡§π‡§æ‡§®‡•ç‡§§‡§∞‡§Æ‡•ç\u0022},{\u0022title\u0022:\u0022\\u003ci class=\\\u0022fas fa-hand-holding-heart\\\u0022\\u003e\\u003c\/i\\u003eDonate Via Vishvas\u0022,\u0022url\u0022:\u0022https:\/\/vvasuki.github.io\/interests\/dharma-via-vishvas\/\u0022}],\u0022title\u0022:\u0022Parts\u0022}}");
    let autocompletePageUrl = "\/notes\/data\/pages.tsv";
    
    var pageRelUrlTree = {};
</script>

    <script>
    
    let pageVars = {};
    pageVars.pageUrlMinusBasePath = "\/notes\/math\/probability\/statistics\/distribution_structure_learning\/1_parameter_estimation\/".replace(basePath, "/");
    pageVars.pageParams = {};
    pageVars.pageSource = "math\/probability\/statistics\/distribution_structure_learning\/1_parameter_estimation.md";
    console.log(pageVars.pageSource);
    var pageDefaults;
    for (let possiblePageDefaults of pageDefaultsList) {
      if (pageVars.pageSource.startsWith(possiblePageDefaults.scope.pathPrefix)) {
        pageDefaults = possiblePageDefaults.values
      }
    }
    
    </script>
    <script src="/notes/webpack_dist/main-bundle.js"></script>
    <script src="/notes/webpack_dist/transliteration-bundle.js"></script>
    <script src="/notes/non_webpack_js/disqus.js"></script>
    <script src="/notes/webpack_dist/ui_lib-bundle.js"></script>
    <link rel="stylesheet" href="/notes/css/fonts.css">
    <link rel="stylesheet" href="/notes/css/@fortawesome/fontawesome-free/css/solid.min.css">
    
    <script type="text/x-mathjax-config">
    MathJax.Hub.Config({"HTML-CSS": { preferredFont: "TeX", availableFonts: ["STIX","TeX"], linebreaks: { automatic:true }, EqnChunk: (MathJax.Hub.Browser.isMobile ? 10 : 50) },
        tex2jax: { inlineMath: [ ["\\$", "\\$"], ["\\(", "\\)"] ], displayMath: [ ["$$","$$"], ["\\[", "\\]"] ], processEscapes: true, ignoreClass: "tex2jax_ignore|dno" },
        TeX: {  noUndefined: { attributes: { mathcolor: "red", mathbackground: "#FFEEEE", mathsize: "90%" } } },
        messageStyle: "none"
    });
</script>
      <script type="text/javascript" id="MathJax-script" async
              src="/notes/non_webpack_js/mathjax/es5/tex-chtml.js?config=TeX-AMS-MML_HTMLorMML">
      </script>
    

    <link rel="stylesheet" href="/notes/css/@fortawesome/fontawesome-free/css/fontawesome.min.css">

    
    <link rel="alternate" hreflang="sa-Deva" href="https://vvasuki.github.io/notes/math/probability/statistics/distribution_structure_learning/1_parameter_estimation/" />
    <link rel="alternate" hreflang="sa-Deva" href="https://vvasuki.github.io/notes/math/probability/statistics/distribution_structure_learning/1_parameter_estimation/?transliteration_target=devanagari" />
    <link rel="alternate" hreflang="sa-Knda" href="https://vvasuki.github.io/notes/math/probability/statistics/distribution_structure_learning/1_parameter_estimation/?transliteration_target=kannada" />
    <link rel="alternate" hreflang="sa-Mlym" href="https://vvasuki.github.io/notes/math/probability/statistics/distribution_structure_learning/1_parameter_estimation/?transliteration_target=malayalam" />
    <link rel="alternate" hreflang="sa-Telu" href="https://vvasuki.github.io/notes/math/probability/statistics/distribution_structure_learning/1_parameter_estimation/?transliteration_target=telugu" />
    <link rel="alternate" hreflang="sa-Taml-t-sa-Taml-m0-superscript" href="https://vvasuki.github.io/notes/math/probability/statistics/distribution_structure_learning/1_parameter_estimation/?transliteration_target=tamil_superscripted" />
    <link rel="alternate" hreflang="sa-Taml" href="https://vvasuki.github.io/notes/math/probability/statistics/distribution_structure_learning/1_parameter_estimation/?transliteration_target=tamil" />
    <link rel="alternate" hreflang="sa-Gran" href="https://vvasuki.github.io/notes/math/probability/statistics/distribution_structure_learning/1_parameter_estimation/?transliteration_target=grantha" />
    <link rel="alternate" hreflang="sa-Gujr" href="https://vvasuki.github.io/notes/math/probability/statistics/distribution_structure_learning/1_parameter_estimation/?transliteration_target=gujarati" />
    <link rel="alternate" hreflang="sa-Orya" href="https://vvasuki.github.io/notes/math/probability/statistics/distribution_structure_learning/1_parameter_estimation/?transliteration_target=oriya" />
    <link rel="alternate" hreflang="sa-Beng-t-sa-Beng-m0-assamese" href="https://vvasuki.github.io/notes/math/probability/statistics/distribution_structure_learning/1_parameter_estimation/?transliteration_target=assamese" />
    <link rel="alternate" hreflang="sa-Beng" href="https://vvasuki.github.io/notes/math/probability/statistics/distribution_structure_learning/1_parameter_estimation/?transliteration_target=bengali" />
    <link rel="alternate" hreflang="sa-Guru" href="https://vvasuki.github.io/notes/math/probability/statistics/distribution_structure_learning/1_parameter_estimation/?transliteration_target=gurmukhi" />
    <link rel="alternate" hreflang="sa-Cyrl" href="https://vvasuki.github.io/notes/math/probability/statistics/distribution_structure_learning/1_parameter_estimation/?transliteration_target=cyrillic" />
    <link rel="alternate" hreflang="sa-Sinh" href="https://vvasuki.github.io/notes/math/probability/statistics/distribution_structure_learning/1_parameter_estimation/?transliteration_target=sinhala" />
    <link rel="alternate" hreflang="sa-Shar" href="https://vvasuki.github.io/notes/math/probability/statistics/distribution_structure_learning/1_parameter_estimation/?transliteration_target=sharada" />
    <link rel="alternate" hreflang="sa-Brah" href="https://vvasuki.github.io/notes/math/probability/statistics/distribution_structure_learning/1_parameter_estimation/?transliteration_target=brahmi" />
    <link rel="alternate" hreflang="sa-Modi" href="https://vvasuki.github.io/notes/math/probability/statistics/distribution_structure_learning/1_parameter_estimation/?transliteration_target=modi" />
    <link rel="alternate" hreflang="sa-Tirh" href="https://vvasuki.github.io/notes/math/probability/statistics/distribution_structure_learning/1_parameter_estimation/?transliteration_target=tirhuta_maithili" />
    <link rel="alternate" hreflang="sa-Latn-t-sa-Zyyy-m0-iast" href="https://vvasuki.github.io/notes/math/probability/statistics/distribution_structure_learning/1_parameter_estimation/?transliteration_target=iast" />
  </head>

  <body class="ma0 bg-near-white">
    
\(
% groupings of objects.
\newcommand{\set}[1]{\left\{ #1 \right\}}
\newcommand{\seq}[1]{\left(#1\right)}
\newcommand{\ang}[1]{\langle#1\rangle}
\newcommand{\tuple}[1]{\left(#1\right)}
\newcommand{\size}[1]{\left| #1\right|}

\newcommand{\comp}{\circ}


% numerical shortcuts.
\newcommand{\abs}[1]{\left| #1\right|}
\newcommand{\floor}[1]{\left\lfloor #1 \right\rfloor}
\newcommand{\ceil}[1]{\left\lceil #1 \right\rceil}

% linear algebra shortcuts.
\newcommand{\change}{\Delta}
\newcommand{\norm}[1]{\left\| #1\right\|}
\newcommand{\dprod}[1]{\langle#1\rangle}
\newcommand{\linspan}[1]{\langle#1\rangle}
\newcommand{\conj}[1]{\overline{#1}}
\newcommand{\der}{\frac{d{dx}}}
\newcommand{\lap}{\Delta}
\newcommand{\kron}{\otimes}
\newcommand{\nperp}{\nvdash}

\newcommand{\mat}[1]{\left[ \begin{smallmatrix}#1 \end{smallmatrix} \right]}

% derivatives and limits
\newcommand{\partder}[2]{\frac{\partial #1}{\partial #2}}
\newcommand{\partdern}[3]{\frac{\partial^{#3 #1}}{\partial #2^{#3}}}
\newcommand{\gradient}{\nabla}
\newcommand{\subdifferential}{\partial}

% Arrows
\newcommand{\diverge}{\nearrow}
\newcommand{\notto}{\nrightarrow}
\newcommand{\up}{\uparrow}
\newcommand{\down}{\downarrow}
% gets and gives are defined!

% ordering operators
\newcommand{\oleq}{\preceq}
\newcommand{\ogeq}{\succeq}

% programming and logic operators
\newcommand{\dfn}{:=}
\newcommand{\assign}{:=}
\newcommand{\co}{\ co\ }
\newcommand{\en}{\ en\ }


% logic operators
\newcommand{\xor}{\oplus}
\newcommand{\Land}{\bigwedge}
\newcommand{\Lor}{\bigvee}
\newcommand{\finish}{\Box}
\newcommand{\contra}{\Rightarrow \Leftarrow}
\newcommand{\iseq}{\stackrel{_?{=}}}


% Set theory
\newcommand{\symdiff}{\Delta}
\newcommand{\setdiff}{\backslash}
\newcommand{\union}{\cup}
\newcommand{\inters}{\cap}
\newcommand{\Union}{\bigcup}
\newcommand{\Inters}{\bigcap}
\newcommand{\nullSet}{\phi}


% graph theory
\newcommand{\nbd}{\Gamma}

% Script alphabets
% For reals, use \Re

% greek letters
\newcommand{\eps}{\epsilon}
\newcommand{\del}{\delta}
\newcommand{\ga}{\alpha}
\newcommand{\gb}{\beta}
\newcommand{\gd}{\del}
\newcommand{\gp}{\pi}
\newcommand{\gf}{\phi}
\newcommand{\gh}{\eta}
\newcommand{\gF}{\Phi}
\newcommand{\gl}{\lambda}
\newcommand{\gm}{\mu}
\newcommand{\gn}{\nu}
\newcommand{\gr}{\rho}
\newcommand{\gs}{\sigma}
\newcommand{\gth}{\theta}
\newcommand{\gx}{\xi}

\newcommand{\sw}{\sigma}
\newcommand{\SW}{\Sigma}
\newcommand{\ew}{\lambda}
\newcommand{\EW}{\Lambda}

\newcommand{\Del}{\Delta}
\newcommand{\gD}{\Delta}
\newcommand{\gG}{\Gamma}
\newcommand{\gO}{\Omega}
\newcommand{\gS}{\Sigma}
\newcommand{\gTh}{\Theta}

% Bold english letters.
\newcommand{\bA}{\mathbf{A}}
\newcommand{\bB}{\mathbf{B}}
\newcommand{\bC}{\mathbf{C}}
\newcommand{\bD}{\mathbf{D}}
\newcommand{\bE}{\mathbf{E}}
\newcommand{\bF}{\mathbf{F}}
\newcommand{\bG}{\mathbf{G}}
\newcommand{\bH}{\mathbf{H}}
\newcommand{\bI}{\mathbf{I}}
\newcommand{\bJ}{\mathbf{J}}
\newcommand{\bK}{\mathbf{K}}
\newcommand{\bL}{\mathbf{L}}
\newcommand{\bM}{\mathbf{M}}
\newcommand{\bN}{\mathbf{N}}
\newcommand{\bO}{\mathbf{O}}
\newcommand{\bP}{\mathbf{P}}
\newcommand{\bQ}{\mathbf{Q}}
\newcommand{\bR}{\mathbf{R}}
\newcommand{\bS}{\mathbf{S}}
\newcommand{\bT}{\mathbf{T}}
\newcommand{\bU}{\mathbf{U}}
\newcommand{\bV}{\mathbf{V}}
\newcommand{\bW}{\mathbf{W}}
\newcommand{\bX}{\mathbf{X}}
\newcommand{\bY}{\mathbf{Y}}
\newcommand{\bZ}{\mathbf{Z}}

\newcommand{\bba}{\mathbf{a}}
\newcommand{\bbb}{\mathbf{b}}
\newcommand{\bbc}{\mathbf{c}}
\newcommand{\bbd}{\mathbf{d}}
\newcommand{\bbe}{\mathbf{e}}
\newcommand{\bbf}{\mathbf{f}}
\newcommand{\bbg}{\mathbf{g}}
\newcommand{\bbh}{\mathbf{h}}
\newcommand{\bbk}{\mathbf{k}}
\newcommand{\bbl}{\mathbf{l}}
\newcommand{\bbm}{\mathbf{m}}
\newcommand{\bbn}{\mathbf{n}}
\newcommand{\bbp}{\mathbf{p}}
\newcommand{\bbq}{\mathbf{q}}
\newcommand{\bbr}{\mathbf{r}}
\newcommand{\bbs}{\mathbf{s}}
\newcommand{\bbt}{\mathbf{t}}
\newcommand{\bbu}{\mathbf{u}}
\newcommand{\bbv}{\mathbf{v}}
\newcommand{\bbw}{\mathbf{w}}
\newcommand{\bbx}{\mathbf{x}}
\newcommand{\bby}{\mathbf{y}}
\newcommand{\bbz}{\mathbf{z}}
\newcommand{\0}{\mathbf{0}}
\newcommand{\1}{\mathbf{1}}

% Caligraphic english alphabet
\newcommand{\cA}{\mathcal{A}}
\newcommand{\cB}{\mathcal{B}}
\newcommand{\cC}{\mathcal{C}}
\newcommand{\cD}{\mathcal{D}}
\newcommand{\cE}{\mathcal{E}}
\newcommand{\cF}{\mathcal{F}}
\newcommand{\cG}{\mathcal{G}}
\newcommand{\cH}{\mathcal{H}}
\newcommand{\cI}{\mathcal{I}}
\newcommand{\cJ}{\mathcal{J}}
\newcommand{\cK}{\mathcal{K}}
\newcommand{\cL}{\mathcal{L}}
\newcommand{\cM}{\mathcal{M}}
\newcommand{\cN}{\mathcal{N}}
\newcommand{\cO}{\mathcal{O}}
\newcommand{\cP}{\mathcal{P}}
\newcommand{\cQ}{\mathcal{Q}}
\newcommand{\cR}{\mathcal{R}}
\newcommand{\cS}{\mathcal{S}}
\newcommand{\cT}{\mathcal{T}}
\newcommand{\cU}{\mathcal{U}}
\newcommand{\cV}{\mathcal{V}}
\newcommand{\cW}{\mathcal{W}}
\newcommand{\cX}{\mathcal{X}}
\newcommand{\cY}{\mathcal{Y}}
\newcommand{\cZ}{\mathcal{Z}}


% Formatting shortcuts
\newcommand{\red}[1]{\textcolor{red}{#1}}
\newcommand{\blue}[1]{\textcolor{blue}{#1}}
\newcommand{\htext}[2]{\texorpdfstring{#1}{#2}}

% Statistics
\newcommand{\distr}{\sim}
\newcommand{\stddev}{\sigma}
\newcommand{\covmatrix}{\Sigma}
\newcommand{\mean}{\mu}
\newcommand{\param}{\theta}
\newcommand{\gthEst}{\hat{\theta}}
\newcommand{\ftr}{\phi}
\newcommand{\est}[1]{\hat{#1}}

% General utility
\newcommand{\todo}[1]{\textbf{[TODO]}] \footnote{TODO: #1}}
\newcommand{\tbc}{[\textbf{Incomplete}]}
\newcommand{\chk}{[\textbf{Check}]}
\newcommand{\why}{[\textbf{Find proof}]}
\newcommand{\opt}[1]{\textit{#1}}

\newcommand{\experience}[1]{[\textbf{Personal Experience}]: #1 \blacktriangle}
\newcommand{\pf}[1]{[\textbf{Proof}]: #1 \Box}
\newcommand{\core}[1]{\textbf{Core Idea}: #1 \Arrowvert}
\newcommand{\example}[1]{\textbf{Example}: #1 \blacktriangle}
\newcommand{\error}[1]{\textbf{Error alert}: #1 \triangle}
\newcommand{\oprob}{[\textbf{OP}]: }


\renewcommand{\~}{\htext{$\sim$}{~}}

\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}


\)



    
<header class="p-1 bg-yellow">
    <nav role="navigation">
      <div id="div_top_bar" class="row">
        <div class="col-md-3 justify-content-start">
          <a  href="/notes/" class="btn col border">
            <i class="fas fa-gopuram ">Vishvas&#39;s notes <br/> 1 Parameter estimation</i>
          </a>
        </div>
        <div id="div_top_bar_right" class="col-md-auto d-flex justify-content-center">
          <form action="/notes/search">
            <input id="titleSearchInputBox" placeholder="‡§∂‡•Ä‡§∞‡•ç‡§∑‡§ø‡§ï‡§æ‡§®‡•ç‡§µ‡§ø‡§∑‡•ç‡§Ø‡§§‡§æ‡§Æ‡•ç" name="s"><i class="btn fas fa-search "></i>
          </form>
          <a href="/notes/custom_site_search/"  class="btn btn-default">Full search</a>
          <select name="transliterationDropdown" size="1" onchange="module_transliteration.updateTransliteration()">
            <option value="devanagari" selected="">‡§∏</option>
            <option value="iast">ƒÅ</option>
            <option value="kannada">‡≤Ö</option>
            <option value="malayalam">‡¥Ö</option>
            <option value="telugu">‡∞ï</option>
            <option value="tamil_superscripted">‡Æï¬≤</option>
            <option value="tamil_extended">‡Æï</option>
            <option value="grantha">ëåÖ</option>
            <option value="gujarati">‡™Ö</option>
            <option value="oriya">‡¨Ö</option>
            <option value="assamese">‡¶Ö‡¶∏</option>
            <option value="bengali">‡¶Ö</option>
            <option value="gurmukhi">‡®Ö</option>
            <option value="cyrillic">–ø—É</option>
            <option value="sinhala">‡∂Ö</option>
            <option value="sharada">ëÜëëáÄëÜ∞</option>
            <option value="brahmi">ëÄÖ</option>
            <option value="modi">ëò¶ëòªëòöëò≤</option>
            <option value="tirhuta_maithili">ëíÅ</option>
          </select>
        </div>
        <div class="row col  d-flex justify-content-center">
          <div><a  name="previousPage" href="" class="btn btn-secondary">###<i class="fas fa-caret-left"></i></a></div>
          <div ><a name="nextPage" href="" class="btn btn-secondary"><i class="fas fa-caret-right"></i> ### </a></div>
        </div>
        <ul id="top-bar-right-custom" class="list-group list-group-horizontal">
        </ul>
      </div>
    </nav>
</header>

    
    <div class="row" name="contentRow">
      
      <aside class="col-md-3 card border " id="sidebar">
        <div id="sidebarTitle" class="card-title bg-light-gray border d-flex justify-content-between" >
          <a name="sidebarToggleLink" data-toggle="collapse" href="#sidebar_body" role="button" aria-expanded="true" aria-controls="sidebar_body" onclick="module_main.default.sidebarToggleHandler()">
            Menu <i class="fas fa-caret-down"></i></a>
        </div>
        <nav class="card-body p-0 collapse show" id="sidebar_body">
          <ul id="displayed_sidebar" class="list pl2 p-2 bg-yellow">
        </ul>
        </nav>
      </aside>
      <main class="col p-3" role="main">
        
<header class='border d-flex justify-content-between'>
    <h1 id="1 Parameter estimation">1 Parameter estimation</h1>
    
    <a id="editLink" class="btn btn-primary"  href="https://github.com/vvasuki/notes/edit/hugo-source/content/math/probability/statistics/distribution_structure_learning/1_parameter_estimation.md"><i class="fas fa-edit"></i></a>
    
</header>
<article>
  <aside id="toc_card" class="card border ">
    <div id="toc_header" class="card-title border d-flex justify-content-between">
        <a data-toggle="collapse" href="#toc_body" role="button" aria-expanded="true" aria-controls="toc_body">
          What's in this page? <i class="fas fa-caret-down"></i> </a>
    </div>
    <div id="toc_body" class="card-body collapse p-0">
      
      <ul id="toc_ul" class="list p-0">
      </ul>
    </div>
  </aside>
  <div id="post_content">
  <h2 id="estimate-parameters-using-statistics">Estimate parameters using statistics</h2>
<p>The distinction between choosing parametric and non-parametric approaches are considered in the decision theory section.</p>
<h3 id="statistic-estimator">Statistic, estimator</h3>
<p>A statistic \(\hat{t} = \hat{g}(X)\) is a function of the sample \(X\); an observable random variable. When it is used to estimate some parameter, it is called an estimator. t can be estimated by estimating \(\gth\).</p>
<h4 id="point-estimation-of-the-parameter">Point estimation of the parameter</h4>
<p>If \(\hat{t}\) tries to approximate \(t\), it is an estimator.</p>
<h3 id="distribution-of-a-statistic">Distribution of a statistic</h3>
<p>Aka Sampling distribution. Standard deviation of sampling distribution called standard error.</p>
<p>Find by manual calculation of probabilities of values of \(\set{X_{i}}\); or by simulation or assume \(\set{X_{i} \distr N(\mean, \stddev^{2})}\), using mgf \(n^{-1}\sum Y_{i} \distr N(\mean, \stddev^{2}/n)\).</p>
<h3 id="summarize-central-tendency">Summarize Central tendency</h3>
<p>Sample and population expectation \<br>
(\(\bar{X}, \mu = E[X]\)), median (m with \(F(m) = 1/2\)) where F is CDF, mode.</p>
<h4 id="am-gm-hm">AM, GM, HM</h4>
<p>Suppose we have \(n\) numbers \((a_i)\). Arithmetic mean is \(n^{-1}\sum_i a_i\): the name reminisces the arithmetic series. Geometric mean is \(\prod_i a_i^(n^{-1})\), and harmonic mean is \(n^{-1}(\sum_i a_i^{-1})^{-1}\).</p>
<p>Using weights \(p_i \in [0, 1]\) such that \(\sum_i p_i = 1\), these quantities can be generalized to define weighted arithmetic, geometric and harmonic means.</p>
<p>\(\mu \geq GM \geq HM\). This and other inequalities are considered in the complex analysis survey.</p>
<h4 id="modeling-accuracy">Modeling accuracy</h4>
<p>If one is trying to use the sample mean to quantify the &lsquo;average&rsquo; phenomenon, it is important to pick the right random variable.</p>
<p>From Tao: &lsquo;For instance, consider the question of what the population density of the United States is. If one does a simple average, dividing the population of the US by the area of the US, one gets a density of about 300 people per square mile, which if the population was spread uniformly, would suggest that each person is about 100 yards from the nearest neighbour. Of course, this does not conform to actual experience. It is true that if one selects a random square mile patch of land from the US at random, it will contain about 300 people in it on the average. However, not all such patches are equally inhabited by humans. If one wants to know what what density the average human in the US sees, rather than the average square mile patch of land, one has to weight each square mile by its population before taking an average. If one does so, the human-weighted population density now increases to about 1400 people per square mile - a significantly different statistic.&rsquo;</p>
<h4 id="combining-arithmetic-means-of-subpopulations">Combining arithmetic means of subpopulations</h4>
<p>(From Tao&rsquo;s blog.) When combining averages of small sub-populations together to form an average of the combined population, one needs to weight each sub-average by the sub-population size in order to not distort the final average. If the sub-populations being averaged over vary, this can then lead to Simpson&rsquo;s paradox.</p>
<p>Eg: it turns out that in most departments, women had a slightly higher success rate in their applications than men, but in the university as a whole, women had a lower success rate. The ultimate reason for this was that women tended to apply to more competitive departments, which lowered their overall average success rate.</p>
<h3 id="other-statistics-and-parameters">Other statistics and parameters</h3>
<h4 id="summarize-variability-or-dispersion">Summarize variability or dispersion</h4>
<p>Sample and population Variance \<br>
(\(S^{2}, \stddev^{2}\)), standard deviation (\(S, \stddev\)) . Variance is average of the squared differences from the Mean. Standard deviation is its square root.</p>
<p>Also, range: max - min.</p>
<h4 id="order-statistics">Order statistics</h4>
<p>max or nth order statistic \(X_{(n)}\), min or first order statistic \(X_{(1)}\), kth smallest sample point \(X_{(k)}\).</p>
<p>\(f_{X_{(k)}}(x) = \frac{n!}{(k-1)!(n-k)!}F_{X}(x)^{k-1}(1-F_{X}(x))^{n-k}f(x)\): consider ways of selecting the kth smallest sample point while ignoring ways of ordering the rest, probability of k-1 of them being smaller and n-k being larger. Treat like any pdf; can find corresponding cdf by integration.</p>
<p>Also, \(f_{X_{(j)}, X_{(k)}}(x, y) =\ \frac{n!}{(j-1)!(k-j-1)!(n-k)!}F_{X}(x)^{k-1}(F(y)-F(x))^{k-j-1}(1-F_{X}(x))^{n-k}f(x)f(y)\).</p>
<h4 id="other-statistics">Other statistics</h4>
<p>Proportion \(\bar{p}\), p: \(n\bar{p} \distr bin(n, p) \to N(np, np(1-p))\); \(\bar{X_{1}} - \bar{X_{2}}, \mean_{1} - \mean_{2}\); \(\bar{p_{1}} - \bar{p_{2}}, p_{1} - p_{2}\). Good pivotal quantity for these: \(\frac{\hat{t}-t}{\stddev_{\hat{t}}} \distr N(0, 1)\).</p>
<h2 id="estimator-properties">Estimator properties</h2>
<p>Properties of good estimators: low bias, low variance, completeness, consistency, sufficiency.</p>
<h3 id="bias">Bias</h3>
<p>\(B(\hat{t}) = E[\hat{t}] - t\). Easy to find unbiased estimator given biased estimator.</p>
<h3 id="mean-square-error-bias-variance-decomposition">Mean square error: Bias variance decomposition</h3>
<p>\(mse(\hat{t}) = E[(t-\hat{t})^{2}] = var[\hat{t}] - B(\hat{t})^{2}\) using bias definition.</p>
<h3 id="relative-efficiency-of-unbiased-estimators">Relative efficiency of unbiased estimators</h3>
<p>\(eff(\hat{t}<em>{1}, \hat{t}<em>2) = var[\hat{t}</em>{2}]/var[\hat{t}</em>{1}]\). To compare variances of estimators.</p>
<h3 id="consistency-of-unbiased-estimators">Consistency of unbiased estimators</h3>
<p>\(\hat{t}<em>{n}\): derived from sample of size n. Consistent if \(\forall \eps: lt</em>{n \to \infty} Pr(|\hat{t}<em>{n} - t| \leq \eps) = 1\). Using Chebyshev&rsquo;s thm, consistency if \(lt</em>{n \to \infty}var[\hat{t}<em>{n}] = 0\). ie: \(\hat{t</em>{n}} \to_{p} t\): \((\hat{t_{n}})\).</p>
<p>Let \(\hat{t_{n}'} \to_{p} t&rsquo;\). \(\hat{t} + \hat{t&rsquo;} \to_{p} t + t;\ \hat{t}\hat{t&rsquo;} \to_{p} tt&rsquo;;\ \hat{t}/\hat{t&rsquo;} \to_{p} t/t&rsquo;\). Also, if \(g() \to R\) continuous, \(g(\hat{t_{n}}) \to g(t)\).</p>
<h3 id="sufficiency-of-unbiased-estimator">Sufficiency of unbiased estimator</h3>
<h4 id="motivation-from-mle">Motivation from MLE</h4>
<p>Given sample vector \(X=x\), suppose that we want to estimate \(T=t\) using estimator \(\hat{T}=\hat{t}\). So, we want to find \(t\) maximizing \(f_{T|X}(t|x) = \frac{f_{X|T}(x|t)f_T(t)}{f_{X}(x)}\).</p>
<p>We may use MLE (assuming all \(t\) equally likely), find t maximizing \(L(t|x) = f_{X|T}(x|t) = f_{X|\hat{T}, T}(x|\hat{t}, t)f_{\hat{T}|T}(\hat{t}|t)\).</p>
<p>So, if \(f_{X|\hat{T}, T}(x|\hat{t}, t)\) is independent of \(T\): \(f_{X|\hat{T}, T}(x|\hat{t}, t) = f_{X|\hat{T}}(x|\hat{t})\), same as maximizing \(f_{\hat{T}|T}(\hat{t}|t)\); can discard \(X=x\) after getting \(T=t\). So, this sufficient statistic summarizes all info in a sample about a parameter.</p>
<p>Not necessarily unbiased. All good estimators, which are unbiased, are functions of sufficient statistic.</p>
<h4 id="to-show-sufficiency-if-distribution-family-known">To show sufficiency if distribution family known</h4>
<p>Show \(f_{X|\hat{T}, T}(x|\hat{t}, t) = f_{X|\hat{T}}(x|\hat{t})\); its form is not a function of t. So, show factorization \(f_{X|T}(x|t) = f_{X|\hat{T}, T}(x|\hat{t}, t)f_{\hat{T}|T}(\hat{t}|t) = g(\hat{t})f(\hat{t},t)\); Or show \(\frac{f_{X|T}(x|t)}{f_{\hat{T}|T}(\hat{t}|t)}\) not a function of t.</p>
<h4 id="to-find-sufficient-statistic">To find sufficient statistic</h4>
<p>Start with \(f_{X|T}(x|t)\), factorize it into \<br>
\(g(&hellip;)f(\hat{t},t)\); in the part which is a \(f(.., t)\), find sufficient statistic \(\hat{t} = h(X)\). Eg: \(f(.., t) = e^{-\sum X_{i}}\), \(\sum X_{i}\) be the minimal sufficient statistic; if \(f(.., t) = e^{-\sum X_{i} - \sum X_{i}^{2}}\) \(\sum X_{i}\) and \(\sum X_{i}^{2}\) are joint sufficient statistics.</p>
<h3 id="statistical-efficiency">Statistical efficiency</h3>
<p>How many observations do you need to get error \(d(\hat{t}, t) &lt; \eps\)?</p>
<h2 id="find-estimator-for-some-parameter">Find estimator for some parameter</h2>
<p>Also see model selection techniques: where you estimate all parameters which define the model from the data.</p>
<h3 id="from-sufficient-statistic">From sufficient statistic</h3>
<p>Find sufficient statistic, then construct unbiased estimator as a function of it.</p>
<h3 id="minimum-variance-unbiased-estimator-mvue">Minimum variance unbiased estimator (MVUE)</h3>
<p>(Rao-Blackwell)\<br>
For parameter t, take any estimator \(\hat{t}\), sufficient statistic U; \(\hat{t&rsquo;} = E[\hat{t}|U]\) (Rao-Blackwellization); then \(E[\hat{t&rsquo;}] = t, var[\hat{t&rsquo;}] \leq var[\hat{t}]\): \(var[\hat{t&rsquo;}] = E[E[\hat{t}|U]^{2}] - t^{2} \leq E[E[\hat{t}^{2}|U]] - t^{2} = var[\hat{t}]\). Rao Blackwellization can&rsquo;t improve variance further.</p>
<p>So, an unbiased estimator which is a function of the sufficient statistic yields MVUE. To find MVUE do this.</p>
<h3 id="method-of-moments">Method of moments</h3>
<p>Assume sample moments are good estimators of population moments. Set \(E[Y|\gth] = \frac{\sum Y_{i}}{n}\); thence get \(f(\gth) = \frac{\sum Y_{i}}{n}\); then solve for \(\gth\). If number of parameters in \(\gth\) is high, find other moments: \(E[Y^{k}|\gth] = \frac{\sum Y_{i}^{k}}{n}\). Result is usually consistent, but not always sufficient.</p>
<h2 id="confidence-interval">Confidence Interval</h2>
<h3 id="definition">Definition</h3>
<p>Find intervals of probable values: confidence intervals \((\hat{t_{1}}, \hat{t_{2}}): Pr(\hat{t_{1}} \leq t \leq \hat{t_{2}}) \leq 1-a\). Can also use one sided intervals using only upper or lower confidence limits: \((-\infty, \hat{t_{2}}), (\hat{t_{1}}, \infty)\).</p>
<p>Contrast with point estimation.</p>
<h3 id="general-procedure">General procedure</h3>
<h4 id="pivotal-quantity-for-estimate">Pivotal quantity for estimate</h4>
<p>Suppose you have a point estimator \(\hat{t}\), which can be expressed as a function of some pivotal quantity \(q\). This quantity has the following property: \(q = g(t, .. )\), which is a function of \(t\) whose distribution function does not depend on \(t\), but may depend on other known/ guessable parameters - like sample size.</p>
<p>Sometimes, the pivotal quantity is the estimator itself.</p>
<h4 id="procedure">Procedure</h4>
<p>For a given sample, one can find \(q\) +++(- the pivotal quantity)+++, find or bound its distribution function \(p\) and finally find suitable confidence interval \(q \in (a, b)\) which translates to the confidence interval estimate: \(t \in (\hat{t_{1}}, \hat{t_{2}})\).</p>
<h3 id="pivotal-quantity-deviation-bounds">Pivotal quantity deviation bounds</h3>
<p>Theoretical calculations can provide deviation bounds for the pivotal quantity distribution - Eg: the use of Chernoff deviation bounds in case of binomial random variables.</p>
<h4 id="by-repeated-sampling">By repeated sampling</h4>
<p>If one can sample repeatedly from the actual distribution, one can estimate the confidence interval for a given estimator.</p>
<h4 id="by-bootstrap-sampling">By Bootstrap sampling</h4>
<h5 id="process">Process</h5>
<p>If one is not able to take repeated samples from the actual distribution, one can repeatedly sample (with replacement) from a uniform distributions over the available sample set.</p>
<p>The justifcation is that this distribution is close to the original distribution, so conclusions drawn from it are not too erroneous.</p>
<h5 id="properties">Properties</h5>
<p>\tbc</p>
<h3 id="pivotal-quantity-for-ratio-of-variances">Pivotal quantity for ratio of variances</h3>
<p>With F sampling distribution: \(\frac{S_{1}^{2}\stddev_{2}^{2}}{S_{2}^{2}\stddev_{1}^{2}} \distr F_{n_{1}-1, n_{2}-1}\).</p>

  </div>
</article>







<aside class="card border" id="section-tree-item-">
    <div class="card-title bg-light-gray border d-flex justify-content-between">
        <a href="https://vvasuki.github.io/notes/math/probability/statistics/distribution_structure_learning/1_parameter_estimation/">1 Parameter estimation </a>
        <a data-toggle="collapse" href="#section-tree-item-body-" role="button" aria-expanded="false" aria-controls="section-tree-item-body-" >‚Ä¶<i class="fas fa-caret-down" class="collapsed"></i> </a>
    </div>
    <nav id="section-tree-item-body-" class="card-body p-0 collapse">
        
        <li>draft: false</li>
        
        <li>iscjklanguage: false</li>
        
        <li>title: 1 Parameter estimation</li>
        
    </nav>
</aside>


      </main>
    </div>
    <footer class="bg-yellow  p-1" role="contentinfo">
  <div id="disqus_thread"></div>
  <div id="div_footer_bar" class="container-fluid d-flex justify-content-between">
    <a class="btn btn-secondary" href="https://github.com/vvasuki/notes/issues/new" >
      ‡§™‡•ç‡§∞‡§§‡§ø‡§∏‡•ç‡§™‡§®‡•ç‡§¶‡§É
    </a>
    <a class="btn btn-secondary" href="http://creativecommons.org/licenses/by/4.0/"><img alt="Creative Commons License" style="border-width:0" src="https://i.creativecommons.org/l/by/4.0/80x15.png" /></a>
    <div class="btn small border">
      Built on 2020 Dec 23 06:12:31 UTC. (<a href="http://google.com/search?q=06%3a12%3a31%20UTC to IST">IST</a>)
    </div>
  <ul id="footer-bar-right-custom" class="list-group list-group-horizontal">
  </ul>
  </div>
</footer>

  </body>
  <script type='text/javascript'>
    
    
    
    module_main.default.onDocumentReadyTasks();
  </script>
</html>
