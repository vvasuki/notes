<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
    
    <title>Vishvas&#39;s notes  | Density estimation</title>
    <meta name="HandheldFriendly" content="True">
    <meta name="MobileOptimized" content="320">

    <meta name="viewport" content="width=device-width,minimum-scale=1">
    <meta name="generator" content="Hugo 0.72.0" />
    
    
      <META NAME="ROBOTS" CONTENT="INDEX, FOLLOW">
    

    <link href="/notes/math/probability/statistics/distribution_structure_learning/density_estimation/" rel="alternate" type="application/rss+xml" title="Vishvas&#39;s notes" />
    <link href="/notes/math/probability/statistics/distribution_structure_learning/density_estimation/" rel="feed" type="application/rss+xml" title="Vishvas&#39;s notes" />

    <meta property="og:title" content="Density estimation" />
<meta property="og:description" content="Importance Fitting a model to observations, ie picking a probability distribution from a family of distributions, is an important component of many statistics tasks where one reasons about uncertainty by explicitly using probability theory. Such tasks are labeled &lsquo;Bayesian inference&rsquo;.
Choosing the distribution family Observe empirical distribution Draw a bar graph, see what the curve looks like.
Given expected values of fns \( \set{E[\ftr_{i(X)}] = \mean_{i} } \) and a base measure h Suppose we want to modify h as little as possible, under KL divergence, so that it has \(E[\ftr(X)] = \mean\)." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://vvasuki.github.io/notes/math/probability/statistics/distribution_structure_learning/density_estimation/" />

<meta itemprop="name" content="Density estimation">
<meta itemprop="description" content="Importance Fitting a model to observations, ie picking a probability distribution from a family of distributions, is an important component of many statistics tasks where one reasons about uncertainty by explicitly using probability theory. Such tasks are labeled &lsquo;Bayesian inference&rsquo;.
Choosing the distribution family Observe empirical distribution Draw a bar graph, see what the curve looks like.
Given expected values of fns \( \set{E[\ftr_{i(X)}] = \mean_{i} } \) and a base measure h Suppose we want to modify h as little as possible, under KL divergence, so that it has \(E[\ftr(X)] = \mean\).">

<meta itemprop="wordCount" content="881">



<meta itemprop="keywords" content="" /><meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Density estimation"/>
<meta name="twitter:description" content="Importance Fitting a model to observations, ie picking a probability distribution from a family of distributions, is an important component of many statistics tasks where one reasons about uncertainty by explicitly using probability theory. Such tasks are labeled &lsquo;Bayesian inference&rsquo;.
Choosing the distribution family Observe empirical distribution Draw a bar graph, see what the curve looks like.
Given expected values of fns \( \set{E[\ftr_{i(X)}] = \mean_{i} } \) and a base measure h Suppose we want to modify h as little as possible, under KL divergence, so that it has \(E[\ftr(X)] = \mean\)."/>

      
    

    <script src="/notes/webpack_dist/dir_tree-bundle.js"></script>
    <script type="text/javascript">
    
    let baseURL = "https:\/\/vvasuki.github.io\/notes\/";
    let basePath = "/" + baseURL.split("/").slice(3).join("/");
    let siteParams = JSON.parse("{\u0022contactlink\u0022:\u0022https:\/\/github.com\/vvasuki\/notes\/issues\/new\u0022,\u0022disqusshortcode\u0022:\u0022vvasuki-site\u0022,\u0022env\u0022:\u0022production\u0022,\u0022githubeditmepathbase\u0022:\u0022https:\/\/github.com\/vvasuki\/notes\/edit\/hugo-source\/content\/\u0022,\u0022mainSections\u0022:[\u0022history\u0022],\u0022mainsections\u0022:[\u0022history\u0022],\u0022math\u0022:true}");
    let pageDefaultsList = JSON.parse("[{\u0022scope\u0022:{\u0022pathPrefix\u0022:\u0022\u0022},\u0022values\u0022:{\u0022comments\u0022:true,\u0022layout\u0022:\u0022page\u0022,\u0022search\u0022:true,\u0022sidebar\u0022:\u0022home_sidebar\u0022}}]");
    let sidebarsData = JSON.parse("{\u0022home_sidebar\u0022:{\u0022contents\u0022:[{\u0022url\u0022:\u0022recdir:\/\/\u0022},{\u0022contents\u0022:[{\u0022title\u0022:\u0022‡§ú‡•ç‡§Ø‡•å‡§§‡§ø‡§∑‡§Æ‡•ç\u0022,\u0022url\u0022:\u0022..\/jyotiSham\/\u0022},{\u0022title\u0022:\u0022‡§∏‡§Ç‡§∏‡•ç‡§ï‡•É‡§§‡§Æ‡•ç\u0022,\u0022url\u0022:\u0022..\/sanskrit\/\u0022},{\u0022title\u0022:\u0022‡§∏‡§Ç‡§∏‡•ç‡§ï‡§æ‡§∞‡§É\u0022,\u0022url\u0022:\u0022..\/saMskAra\/\u0022},{\u0022title\u0022:\u0022Notes Home\u0022,\u0022url\u0022:\u0022..\/notes\/\u0022},{\u0022title\u0022:\u0022‡§µ‡•á‡§¶‡§æ‡§É\u0022,\u0022url\u0022:\u0022..\/vedAH\/\u0022},{\u0022title\u0022:\u0022‡§™‡•Å‡§∞‡§æ‡§£‡§Æ‡•ç\u0022,\u0022url\u0022:\u0022..\/purANam\/\u0022},{\u0022title\u0022:\u0022‡§ï‡§æ‡§µ‡•ç‡§Ø‡§Æ‡•ç\u0022,\u0022url\u0022:\u0022..\/kAvyam\/\u0022},{\u0022title\u0022:\u0022‡§Æ‡•Ä‡§Æ‡§æ‡§Ç‡§∏‡§æ\u0022,\u0022url\u0022:\u0022..\/mImAMsA\/\u0022},{\u0022title\u0022:\u0022‡§§‡•ç‡§∞‡§ø‡§™‡§ø‡§ü‡§ï‡§Æ‡•ç\u0022,\u0022url\u0022:\u0022..\/tipiTaka\/\u0022},{\u0022title\u0022:\u0022‡§™‡§æ‡§≥‡§Ø‡§É\u0022,\u0022url\u0022:\u0022..\/pALi\/\u0022},{\u0022title\u0022:\u0022Vishvas\u0027s home page\u0022,\u0022url\u0022:\u0022\/..\/\u0022}],\u0022title\u0022:\u0022‡§∏‡§ô‡•ç‡§ó‡•ç‡§∞‡§π‡§æ‡§®‡•ç‡§§‡§∞‡§Æ‡•ç\u0022},{\u0022title\u0022:\u0022\\u003ci class=\\\u0022fas fa-hand-holding-heart\\\u0022\\u003e\\u003c\/i\\u003eDonate Via Vishvas\u0022,\u0022url\u0022:\u0022https:\/\/vvasuki.github.io\/interests\/dharma-via-vishvas\/\u0022}],\u0022title\u0022:\u0022Parts\u0022}}");
    let autocompletePageUrl = "\/notes\/data\/pages.tsv";
    
    var pageRelUrlTree = {};
</script>

    <script>
    
    let pageVars = {};
    pageVars.pageUrlMinusBasePath = "\/notes\/math\/probability\/statistics\/distribution_structure_learning\/density_estimation\/".replace(basePath, "/");
    pageVars.pageParams = {};
    pageVars.pageSource = "math\/probability\/statistics\/distribution_structure_learning\/density_estimation.md";
    console.log(pageVars.pageSource);
    var pageDefaults;
    for (let possiblePageDefaults of pageDefaultsList) {
      if (pageVars.pageSource.startsWith(possiblePageDefaults.scope.pathPrefix)) {
        pageDefaults = possiblePageDefaults.values
      }
    }
    
    </script>
    <script src="/notes/webpack_dist/main-bundle.js"></script>
    <script src="/notes/webpack_dist/transliteration-bundle.js"></script>
    <script src="/notes/non_webpack_js/disqus.js"></script>
    <script src="/notes/webpack_dist/ui_lib-bundle.js"></script>
    <link rel="stylesheet" href="/notes/css/fonts.css">
    <link rel="stylesheet" href="/notes/css/@fortawesome/fontawesome-free/css/solid.min.css">
    
    <script type="text/x-mathjax-config">
    MathJax.Hub.Config({"HTML-CSS": { preferredFont: "TeX", availableFonts: ["STIX","TeX"], linebreaks: { automatic:true }, EqnChunk: (MathJax.Hub.Browser.isMobile ? 10 : 50) },
        tex2jax: { inlineMath: [ ["\\$", "\\$"], ["\\(", "\\)"] ], displayMath: [ ["$$","$$"], ["\\[", "\\]"] ], processEscapes: true, ignoreClass: "tex2jax_ignore|dno" },
        TeX: {  noUndefined: { attributes: { mathcolor: "red", mathbackground: "#FFEEEE", mathsize: "90%" } } },
        messageStyle: "none"
    });
</script>
      <script type="text/javascript" id="MathJax-script" async
              src="/notes/non_webpack_js/mathjax/es5/tex-chtml.js?config=TeX-AMS-MML_HTMLorMML">
      </script>
    

    <link rel="stylesheet" href="/notes/css/@fortawesome/fontawesome-free/css/fontawesome.min.css">

    
    <link rel="alternate" hreflang="sa-Deva" href="https://vvasuki.github.io/notes/math/probability/statistics/distribution_structure_learning/density_estimation/" />
    <link rel="alternate" hreflang="sa-Deva" href="https://vvasuki.github.io/notes/math/probability/statistics/distribution_structure_learning/density_estimation/?transliteration_target=devanagari" />
    <link rel="alternate" hreflang="sa-Knda" href="https://vvasuki.github.io/notes/math/probability/statistics/distribution_structure_learning/density_estimation/?transliteration_target=kannada" />
    <link rel="alternate" hreflang="sa-Mlym" href="https://vvasuki.github.io/notes/math/probability/statistics/distribution_structure_learning/density_estimation/?transliteration_target=malayalam" />
    <link rel="alternate" hreflang="sa-Telu" href="https://vvasuki.github.io/notes/math/probability/statistics/distribution_structure_learning/density_estimation/?transliteration_target=telugu" />
    <link rel="alternate" hreflang="sa-Taml-t-sa-Taml-m0-superscript" href="https://vvasuki.github.io/notes/math/probability/statistics/distribution_structure_learning/density_estimation/?transliteration_target=tamil_superscripted" />
    <link rel="alternate" hreflang="sa-Taml" href="https://vvasuki.github.io/notes/math/probability/statistics/distribution_structure_learning/density_estimation/?transliteration_target=tamil" />
    <link rel="alternate" hreflang="sa-Gran" href="https://vvasuki.github.io/notes/math/probability/statistics/distribution_structure_learning/density_estimation/?transliteration_target=grantha" />
    <link rel="alternate" hreflang="sa-Gujr" href="https://vvasuki.github.io/notes/math/probability/statistics/distribution_structure_learning/density_estimation/?transliteration_target=gujarati" />
    <link rel="alternate" hreflang="sa-Orya" href="https://vvasuki.github.io/notes/math/probability/statistics/distribution_structure_learning/density_estimation/?transliteration_target=oriya" />
    <link rel="alternate" hreflang="sa-Beng-t-sa-Beng-m0-assamese" href="https://vvasuki.github.io/notes/math/probability/statistics/distribution_structure_learning/density_estimation/?transliteration_target=assamese" />
    <link rel="alternate" hreflang="sa-Beng" href="https://vvasuki.github.io/notes/math/probability/statistics/distribution_structure_learning/density_estimation/?transliteration_target=bengali" />
    <link rel="alternate" hreflang="sa-Guru" href="https://vvasuki.github.io/notes/math/probability/statistics/distribution_structure_learning/density_estimation/?transliteration_target=gurmukhi" />
    <link rel="alternate" hreflang="sa-Cyrl" href="https://vvasuki.github.io/notes/math/probability/statistics/distribution_structure_learning/density_estimation/?transliteration_target=cyrillic" />
    <link rel="alternate" hreflang="sa-Sinh" href="https://vvasuki.github.io/notes/math/probability/statistics/distribution_structure_learning/density_estimation/?transliteration_target=sinhala" />
    <link rel="alternate" hreflang="sa-Shar" href="https://vvasuki.github.io/notes/math/probability/statistics/distribution_structure_learning/density_estimation/?transliteration_target=sharada" />
    <link rel="alternate" hreflang="sa-Brah" href="https://vvasuki.github.io/notes/math/probability/statistics/distribution_structure_learning/density_estimation/?transliteration_target=brahmi" />
    <link rel="alternate" hreflang="sa-Modi" href="https://vvasuki.github.io/notes/math/probability/statistics/distribution_structure_learning/density_estimation/?transliteration_target=modi" />
    <link rel="alternate" hreflang="sa-Tirh" href="https://vvasuki.github.io/notes/math/probability/statistics/distribution_structure_learning/density_estimation/?transliteration_target=tirhuta_maithili" />
    <link rel="alternate" hreflang="sa-Latn-t-sa-Zyyy-m0-iast" href="https://vvasuki.github.io/notes/math/probability/statistics/distribution_structure_learning/density_estimation/?transliteration_target=iast" />
  </head>

  <body class="ma0 bg-near-white">
    
\(
% groupings of objects.
\newcommand{\set}[1]{\left\{ #1 \right\}}
\newcommand{\seq}[1]{\left(#1\right)}
\newcommand{\ang}[1]{\langle#1\rangle}
\newcommand{\tuple}[1]{\left(#1\right)}
\newcommand{\size}[1]{\left| #1\right|}

\newcommand{\comp}{\circ}


% numerical shortcuts.
\newcommand{\abs}[1]{\left| #1\right|}
\newcommand{\floor}[1]{\left\lfloor #1 \right\rfloor}
\newcommand{\ceil}[1]{\left\lceil #1 \right\rceil}

% linear algebra shortcuts.
\newcommand{\change}{\Delta}
\newcommand{\norm}[1]{\left\| #1\right\|}
\newcommand{\dprod}[1]{\langle#1\rangle}
\newcommand{\linspan}[1]{\langle#1\rangle}
\newcommand{\conj}[1]{\overline{#1}}
\newcommand{\der}{\frac{d{dx}}}
\newcommand{\lap}{\Delta}
\newcommand{\kron}{\otimes}
\newcommand{\nperp}{\nvdash}

\newcommand{\mat}[1]{\left[ \begin{smallmatrix}#1 \end{smallmatrix} \right]}

% derivatives and limits
\newcommand{\partder}[2]{\frac{\partial #1}{\partial #2}}
\newcommand{\partdern}[3]{\frac{\partial^{#3 #1}}{\partial #2^{#3}}}
\newcommand{\gradient}{\nabla}
\newcommand{\subdifferential}{\partial}

% Arrows
\newcommand{\diverge}{\nearrow}
\newcommand{\notto}{\nrightarrow}
\newcommand{\up}{\uparrow}
\newcommand{\down}{\downarrow}
% gets and gives are defined!

% ordering operators
\newcommand{\oleq}{\preceq}
\newcommand{\ogeq}{\succeq}

% programming and logic operators
\newcommand{\dfn}{:=}
\newcommand{\assign}{:=}
\newcommand{\co}{\ co\ }
\newcommand{\en}{\ en\ }


% logic operators
\newcommand{\xor}{\oplus}
\newcommand{\Land}{\bigwedge}
\newcommand{\Lor}{\bigvee}
\newcommand{\finish}{\Box}
\newcommand{\contra}{\Rightarrow \Leftarrow}
\newcommand{\iseq}{\stackrel{_?{=}}}


% Set theory
\newcommand{\symdiff}{\Delta}
\newcommand{\setdiff}{\backslash}
\newcommand{\union}{\cup}
\newcommand{\inters}{\cap}
\newcommand{\Union}{\bigcup}
\newcommand{\Inters}{\bigcap}
\newcommand{\nullSet}{\phi}


% graph theory
\newcommand{\nbd}{\Gamma}

% Script alphabets
% For reals, use \Re

% greek letters
\newcommand{\eps}{\epsilon}
\newcommand{\del}{\delta}
\newcommand{\ga}{\alpha}
\newcommand{\gb}{\beta}
\newcommand{\gd}{\del}
\newcommand{\gp}{\pi}
\newcommand{\gf}{\phi}
\newcommand{\gh}{\eta}
\newcommand{\gF}{\Phi}
\newcommand{\gl}{\lambda}
\newcommand{\gm}{\mu}
\newcommand{\gn}{\nu}
\newcommand{\gr}{\rho}
\newcommand{\gs}{\sigma}
\newcommand{\gth}{\theta}
\newcommand{\gx}{\xi}

\newcommand{\sw}{\sigma}
\newcommand{\SW}{\Sigma}
\newcommand{\ew}{\lambda}
\newcommand{\EW}{\Lambda}

\newcommand{\Del}{\Delta}
\newcommand{\gD}{\Delta}
\newcommand{\gG}{\Gamma}
\newcommand{\gO}{\Omega}
\newcommand{\gS}{\Sigma}
\newcommand{\gTh}{\Theta}

% Bold english letters.
\newcommand{\bA}{\mathbf{A}}
\newcommand{\bB}{\mathbf{B}}
\newcommand{\bC}{\mathbf{C}}
\newcommand{\bD}{\mathbf{D}}
\newcommand{\bE}{\mathbf{E}}
\newcommand{\bF}{\mathbf{F}}
\newcommand{\bG}{\mathbf{G}}
\newcommand{\bH}{\mathbf{H}}
\newcommand{\bI}{\mathbf{I}}
\newcommand{\bJ}{\mathbf{J}}
\newcommand{\bK}{\mathbf{K}}
\newcommand{\bL}{\mathbf{L}}
\newcommand{\bM}{\mathbf{M}}
\newcommand{\bN}{\mathbf{N}}
\newcommand{\bO}{\mathbf{O}}
\newcommand{\bP}{\mathbf{P}}
\newcommand{\bQ}{\mathbf{Q}}
\newcommand{\bR}{\mathbf{R}}
\newcommand{\bS}{\mathbf{S}}
\newcommand{\bT}{\mathbf{T}}
\newcommand{\bU}{\mathbf{U}}
\newcommand{\bV}{\mathbf{V}}
\newcommand{\bW}{\mathbf{W}}
\newcommand{\bX}{\mathbf{X}}
\newcommand{\bY}{\mathbf{Y}}
\newcommand{\bZ}{\mathbf{Z}}

\newcommand{\bba}{\mathbf{a}}
\newcommand{\bbb}{\mathbf{b}}
\newcommand{\bbc}{\mathbf{c}}
\newcommand{\bbd}{\mathbf{d}}
\newcommand{\bbe}{\mathbf{e}}
\newcommand{\bbf}{\mathbf{f}}
\newcommand{\bbg}{\mathbf{g}}
\newcommand{\bbh}{\mathbf{h}}
\newcommand{\bbk}{\mathbf{k}}
\newcommand{\bbl}{\mathbf{l}}
\newcommand{\bbm}{\mathbf{m}}
\newcommand{\bbn}{\mathbf{n}}
\newcommand{\bbp}{\mathbf{p}}
\newcommand{\bbq}{\mathbf{q}}
\newcommand{\bbr}{\mathbf{r}}
\newcommand{\bbs}{\mathbf{s}}
\newcommand{\bbt}{\mathbf{t}}
\newcommand{\bbu}{\mathbf{u}}
\newcommand{\bbv}{\mathbf{v}}
\newcommand{\bbw}{\mathbf{w}}
\newcommand{\bbx}{\mathbf{x}}
\newcommand{\bby}{\mathbf{y}}
\newcommand{\bbz}{\mathbf{z}}
\newcommand{\0}{\mathbf{0}}
\newcommand{\1}{\mathbf{1}}

% Caligraphic english alphabet
\newcommand{\cA}{\mathcal{A}}
\newcommand{\cB}{\mathcal{B}}
\newcommand{\cC}{\mathcal{C}}
\newcommand{\cD}{\mathcal{D}}
\newcommand{\cE}{\mathcal{E}}
\newcommand{\cF}{\mathcal{F}}
\newcommand{\cG}{\mathcal{G}}
\newcommand{\cH}{\mathcal{H}}
\newcommand{\cI}{\mathcal{I}}
\newcommand{\cJ}{\mathcal{J}}
\newcommand{\cK}{\mathcal{K}}
\newcommand{\cL}{\mathcal{L}}
\newcommand{\cM}{\mathcal{M}}
\newcommand{\cN}{\mathcal{N}}
\newcommand{\cO}{\mathcal{O}}
\newcommand{\cP}{\mathcal{P}}
\newcommand{\cQ}{\mathcal{Q}}
\newcommand{\cR}{\mathcal{R}}
\newcommand{\cS}{\mathcal{S}}
\newcommand{\cT}{\mathcal{T}}
\newcommand{\cU}{\mathcal{U}}
\newcommand{\cV}{\mathcal{V}}
\newcommand{\cW}{\mathcal{W}}
\newcommand{\cX}{\mathcal{X}}
\newcommand{\cY}{\mathcal{Y}}
\newcommand{\cZ}{\mathcal{Z}}


% Formatting shortcuts
\newcommand{\red}[1]{\textcolor{red}{#1}}
\newcommand{\blue}[1]{\textcolor{blue}{#1}}
\newcommand{\htext}[2]{\texorpdfstring{#1}{#2}}

% Statistics
\newcommand{\distr}{\sim}
\newcommand{\stddev}{\sigma}
\newcommand{\covmatrix}{\Sigma}
\newcommand{\mean}{\mu}
\newcommand{\param}{\theta}
\newcommand{\gthEst}{\hat{\theta}}
\newcommand{\ftr}{\phi}
\newcommand{\est}[1]{\hat{#1}}

% General utility
\newcommand{\todo}[1]{\textbf{[TODO]}] \footnote{TODO: #1}}
\newcommand{\tbc}{[\textbf{Incomplete}]}
\newcommand{\chk}{[\textbf{Check}]}
\newcommand{\why}{[\textbf{Find proof}]}
\newcommand{\opt}[1]{\textit{#1}}

\newcommand{\experience}[1]{[\textbf{Personal Experience}]: #1 \blacktriangle}
\newcommand{\pf}[1]{[\textbf{Proof}]: #1 \Box}
\newcommand{\core}[1]{\textbf{Core Idea}: #1 \Arrowvert}
\newcommand{\example}[1]{\textbf{Example}: #1 \blacktriangle}
\newcommand{\error}[1]{\textbf{Error alert}: #1 \triangle}
\newcommand{\oprob}{[\textbf{OP}]: }


\renewcommand{\~}{\htext{$\sim$}{~}}

\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}


\)



    
<header class="p-1 bg-yellow">
    <nav role="navigation">
      <div id="div_top_bar" class="row">
        <div class="col-md-3 justify-content-start">
          <a  href="/notes/" class="btn col border">
            <i class="fas fa-gopuram ">Vishvas&#39;s notes <br/> Density estimation</i>
          </a>
        </div>
        <div id="div_top_bar_right" class="col-md-auto d-flex justify-content-center">
          <form action="/notes/search">
            <input id="titleSearchInputBox" placeholder="‡§∂‡•Ä‡§∞‡•ç‡§∑‡§ø‡§ï‡§æ‡§®‡•ç‡§µ‡§ø‡§∑‡•ç‡§Ø‡§§‡§æ‡§Æ‡•ç" name="s"><i class="btn fas fa-search "></i>
          </form>
          <a href="/notes/custom_site_search/"  class="btn btn-default">Full search</a>
          <select name="transliterationDropdown" size="1" onchange="module_transliteration.updateTransliteration()">
            <option value="devanagari" selected="">‡§∏</option>
            <option value="iast">ƒÅ</option>
            <option value="kannada">‡≤Ö</option>
            <option value="malayalam">‡¥Ö</option>
            <option value="telugu">‡∞ï</option>
            <option value="tamil_superscripted">‡Æï¬≤</option>
            <option value="tamil_extended">‡Æï</option>
            <option value="grantha">ëåÖ</option>
            <option value="gujarati">‡™Ö</option>
            <option value="oriya">‡¨Ö</option>
            <option value="assamese">‡¶Ö‡¶∏</option>
            <option value="bengali">‡¶Ö</option>
            <option value="gurmukhi">‡®Ö</option>
            <option value="cyrillic">–ø—É</option>
            <option value="sinhala">‡∂Ö</option>
            <option value="sharada">ëÜëëáÄëÜ∞</option>
            <option value="brahmi">ëÄÖ</option>
            <option value="modi">ëò¶ëòªëòöëò≤</option>
            <option value="tirhuta_maithili">ëíÅ</option>
          </select>
        </div>
        <div class="row col  d-flex justify-content-center">
          <div><a  name="previousPage" href="" class="btn btn-secondary">###<i class="fas fa-caret-left"></i></a></div>
          <div ><a name="nextPage" href="" class="btn btn-secondary"><i class="fas fa-caret-right"></i> ### </a></div>
        </div>
        <ul id="top-bar-right-custom" class="list-group list-group-horizontal">
        </ul>
      </div>
    </nav>
</header>

    
    <div class="row" name="contentRow">
      
      <aside class="col-md-3 card border " id="sidebar">
        <div id="sidebarTitle" class="card-title bg-light-gray border d-flex justify-content-between" >
          <a name="sidebarToggleLink" data-toggle="collapse" href="#sidebar_body" role="button" aria-expanded="true" aria-controls="sidebar_body" onclick="module_main.default.sidebarToggleHandler()">
            Menu <i class="fas fa-caret-down"></i></a>
        </div>
        <nav class="card-body p-0 collapse show" id="sidebar_body">
          <ul id="displayed_sidebar" class="list pl2 p-2 bg-yellow">
        </ul>
        </nav>
      </aside>
      <main class="col p-3" role="main">
        
<header class='border d-flex justify-content-between'>
    <h1 id="Density estimation">Density estimation</h1>
    
    <a id="editLink" class="btn btn-primary"  href="https://github.com/vvasuki/notes/edit/hugo-source/content/math/probability/statistics/distribution_structure_learning/density_estimation.md"><i class="fas fa-edit"></i></a>
    
</header>
<article>
  <aside id="toc_card" class="card border ">
    <div id="toc_header" class="card-title border d-flex justify-content-between">
        <a data-toggle="collapse" href="#toc_body" role="button" aria-expanded="true" aria-controls="toc_body">
          What's in this page? <i class="fas fa-caret-down"></i> </a>
    </div>
    <div id="toc_body" class="card-body collapse p-0">
      
      <ul id="toc_ul" class="list p-0">
      </ul>
    </div>
  </aside>
  <div id="post_content">
  <h2 id="importance">Importance</h2>
<p>Fitting a model to observations, ie picking a probability distribution from a family of distributions, is an important component of many statistics tasks where one reasons about uncertainty by explicitly using probability theory. Such tasks are labeled &lsquo;Bayesian inference&rsquo;.</p>
<h2 id="choosing-the-distribution-family">Choosing the distribution family</h2>
<h3 id="observe-empirical-distribution">Observe empirical distribution</h3>
<p>Draw a bar graph, see what the curve looks like.</p>
<h3 id="given-expected-values-of-fns--seteftr_ix--mean_i---and-a-base-measure-h">Given expected values of fns \( \set{E[\ftr_{i(X)}] = \mean_{i} } \) and a base measure h</h3>
<p>Suppose we want to modify h as little as possible, under KL divergence, so that it has \(E[\ftr(X)] = \mean\). If h is U, then this is same as finding a maximum entropy distribution with \(E[\ftr(X)] = \mean\). Then, the solution belongs to the exponential family generated by h and \(\ftr()\): see probabilistic models ref.</p>
<h3 id="given-dependence-among-features">Given dependence among features</h3>
<p>Use graphical models - see probability ref.</p>
<h2 id="parametric-density-estimation">Parametric density estimation</h2>
<p>Described in a separate chapter.</p>
<h2 id="non-parametric-probability-density-estimation">Non parametric Probability Density estimation</h2>
<p>Estimate distribution on input space using \(N\) samples \((x_{i})\), without limiting attention to a certain set of distributions.</p>
<h3 id="histogram-and-the-kernel-histogram">Histogram and the Kernel histogram</h3>
<p>A distribution from bar-graph of frequency vs input interval. Can simply use a histogram.</p>
<h3 id="kernel-density-estimation">Kernel density estimation</h3>
<p>(Parzen). \(p(x) = \frac{1}{Nh}\sum_{i=1}^{N} K(\frac{x-x_{i}}{h})\), for kernel K. A smoothening of the histogram using kernels.</p>
<h4 id="kernel-function-for-density-estimation">Kernel function for density estimation</h4>
<p>Non negative real valued integrable K(x) satisfies: \(\int_{-\infty}^{+\infty}K(u)du = 1\) (ensures PDF qualities during density estimation); \(K(-u) = u\) (ensures mean of the PDF is the data point during density estimation). So, K(x) akin to kernel of an integral operator: see functional analysis ref.</p>
<h4 id="using-gaussian-radial-basis-functions">Using Gaussian radial basis functions</h4>
<p>Aka Gaussian kernel.\<br>
\(K(u) = \frac{1}{2\pi}e^{-\frac{u^{2}}{2}}\): Gaussian function with mean 0, variance 1.</p>
<p>Taking 1 Gaussian distribution/ adding 1 bump for each data point. h, controlling the variance of the bump, called the smoothing parameter/ bandwidth.</p>
<p>+++(Can approximate any distribution by mixture of Gaussians!)+++</p>
<h2 id="estimate-probability-measures">Estimate probability measures</h2>
<h3 id="use-empirical-measures">Use empirical measures</h3>
<h4 id="empirical-measure">Empirical measure</h4>
<p>The estimated measure using \(n\) samples for a given function is \(v_{n}(f| D) = n^{-1} \sum I_{f}(X_i)\), where \(D = \set{X_i}\) is a set of samples drawn iid from \(v\).</p>
<h4 id="goodness-of-estimate-single-event">Goodness of estimate: single event</h4>
<p>By law of large numbers, as \(\lim_{n \to \infty} E[\frac{\sum f(X_{i})}{n}] = E_X[f(X)] = v(f)\). Also, we can use the Hoeffding inequality (see probabilistic models survey) to bound \(Pr_D(|v(f) - v_n(f|D)| \geq \eps)\) and see that it decreases exponentially with increasing \(n\) and \(\eps\).</p>
<h5 id="bound-variability-in-estimate">Bound variability in estimate</h5>
<p>From central limit theorem, we know that, as \(n \to \infty\), the sample distribution approaches \(N(v(f), \frac{\stddev}{\sqrt{n}})\).</p>
<p>Also, we can use: \(Pr_{D, D&rsquo;}(|v_n(f|D) - v_n(f|D&rsquo;)|\geq \eps) \leq Pr_{D, D&rsquo;}(|v_n(f|D) - v(f)|\geq \eps/2 \lor |v_n(f|D&rsquo;) - v(f)|\geq \eps/2) = 2Pr(|v_n(f|D) - v(f)| \leq \eps/2)\) and use the Hoeffding inequality again.</p>
<h4 id="goodness-of-estimate-for-a-class-of-events">Goodness of estimate for a class of events</h4>
<p>Let \(F = \set{f}\) be a class of events (or binary functions) defined on the input space \(X\), on which \(v\) is a measure. Let \(E_{F}(m)\) be max dichotomy count: see boolean functions survey.</p>
<p>(Vapnik, Chervonenkis).</p>
<p>Then \(Pr(\sup_{f \in F} |v_n(f|D) - v(f)| &gt; \eps) \leq 8 E_{F}(n)exp(\frac{-n\eps^{2}}{32})\).</p>
<div class="proof">  If we were to use the union bound and the Hoeffding inequality naively, we would have a factor of \\(2|F| >> 8E_F(n)\\) on the RHS. So, we want to get to a point where we need only take the union bound over a small subset of \\(F\\)</div>
<p>So, first we show that \<br>
\(Pr_D(\sup_{f \in F} |v_n(f|D) - v(f)| &gt; \eps) \leq 2Pr_{D, D&rsquo;}(\sup_{f \in F} |v_n(f|D) - v_n(f|D&rsquo;)| &gt; \eps/2)\), for sample set \(D&rsquo;\) acquired in the same way as \(D\). Lemma <div class="proof">\(Pr_{D, D&rsquo;}(\sup_{f \in F} |v_n(f|D) - v_n(f|D&rsquo;)| &gt; \eps/2) \geq Pr_{D}(\sup_{f \in F} |v_n(f|D) - v(f)| &gt; \eps)Pr_{D&rsquo;}(|v_n(f|D&rsquo;) - v_n(f)| &lt; \eps/2| |v_n(f'|D) - v(f&rsquo;)| &gt; \eps)\); and the latter factor can be seen to be small: \(&lt;1/2\) using Chebyshev inequality. This is called the &lsquo;ghost sample technique&rsquo;.</div></p>
<p>So, now we need only bound \(Pr_{D, D&rsquo;}(\sup_{f \in F} |v_n(f|D) - v_n(f|D&rsquo;)| &gt; \eps/2)\). One way to deal with this is to take the union bound now over the set of all dichotomies induced over \(D \union D&rsquo;\) to get the bound: \(E_F(2n)Pr(D, D&rsquo;)( |v_n(f|D) - v_n(f|D&rsquo;)| &gt; \eps/2)\), which can then be bounded as described elsewhere.}</p>
<h5 id="sharper-bound">Sharper bound</h5>
<p>The sharper bound we require can be obtained using a different analysis, involving \(E_F(n)\) instead. The process of picking \(D, D&rsquo;\) and calculating \(n^{-1}|\sum_i f(X_i) - \sum_i f(X&rsquo;<em>i)|\) is equivalent to the process of picking \(D, D&rsquo;\), and then picking \(n\) bits \(s\), and then finding \(n^{-1}|s_i(\sum_i f(X_i) - \sum_i f(X&rsquo;<em>i))|\) : in other words, we do this \(n\) times: pick a pair of points and assign it to \(D\) and \(D&rsquo;\) at random. So, we bound \(Pr</em>{D, D&rsquo;,s }(\sup</em>{f \in F} n^{-1}|s_i(\sum_i f(X_i) - \sum_i f(X&rsquo;<em>i))|&gt; \eps/2) \leq Pr</em>{D, D&rsquo;,s }(\sup_{f \in F} [|n^{-1}\sum_i s_if(X_i)| \geq \eps/4 \lor |n^{-1}\sum_i -s_if(X&rsquo;_i)| \geq \eps/4]) = 2Pr_{D, s}(\sup_{f \in F} |n^{-1}\sum_i s_if(X_i)| \geq \eps/4)\). A union bound over \(E_F(n)\) members of \(F\) and a Hoeffding inequality can now be applied to bound this.</p>
<h3 id="estimate-cdf-using-empirical-cdf">Estimate CDF using empirical CDF</h3>
<p>(Glivenko Cantelli). Pick \(\set{Z_i} \distr F\), the CDF. \<br>
Then \(Pr(\sup_z |F_n(z) - F(z)| &gt; \eps) \leq 8(n+1)exp(\frac{-n\eps^{2}}{32})\). Pf: Apply VCD theorem with open intervals as classifiers.</p>
<p>Answer to: What \(n\) do you need to achieve low error? Thence Borel - Cantelli: \(\lim_{n \to \infty} \sup |F_n(z) - F(z)| = 0\) with probability 1.</p>

  </div>
</article>







<aside class="card border" id="section-tree-item-">
    <div class="card-title bg-light-gray border d-flex justify-content-between">
        <a href="https://vvasuki.github.io/notes/math/probability/statistics/distribution_structure_learning/density_estimation/">Density estimation </a>
        <a data-toggle="collapse" href="#section-tree-item-body-" role="button" aria-expanded="false" aria-controls="section-tree-item-body-" >‚Ä¶<i class="fas fa-caret-down" class="collapsed"></i> </a>
    </div>
    <nav id="section-tree-item-body-" class="card-body p-0 collapse">
        
        <li>draft: false</li>
        
        <li>iscjklanguage: false</li>
        
        <li>title: Density estimation</li>
        
    </nav>
</aside>


      </main>
    </div>
    <footer class="bg-yellow  p-1" role="contentinfo">
  <div id="disqus_thread"></div>
  <div id="div_footer_bar" class="container-fluid d-flex justify-content-between">
    <a class="btn btn-secondary" href="https://github.com/vvasuki/notes/issues/new" >
      ‡§™‡•ç‡§∞‡§§‡§ø‡§∏‡•ç‡§™‡§®‡•ç‡§¶‡§É
    </a>
    <a class="btn btn-secondary" href="http://creativecommons.org/licenses/by/4.0/"><img alt="Creative Commons License" style="border-width:0" src="https://i.creativecommons.org/l/by/4.0/80x15.png" /></a>
    <div class="btn small border">
      Built on 2020 Dec 23 06:22:02 UTC. (<a href="http://google.com/search?q=06%3a22%3a02%20UTC to IST">IST</a>)
    </div>
  <ul id="footer-bar-right-custom" class="list-group list-group-horizontal">
  </ul>
  </div>
</footer>

  </body>
  <script type='text/javascript'>
    
    
    
    module_main.default.onDocumentReadyTasks();
  </script>
</html>
